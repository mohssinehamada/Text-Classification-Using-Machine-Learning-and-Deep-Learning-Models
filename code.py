# -*- coding: utf-8 -*-
"""Hmada_Mouhcine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LKCcUyJ9g9IOgHDmtVu1K32dZ6aEwZdi
"""

!pip install datasets

!pip install transformers tensorflow

import re
import string
import nltk
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from nltk.corpus import stopwords

from datasets import load_dataset
import pandas as pd

nltk.download('stopwords')

""" Load and Prepare the Dataset"""

dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')

dataset

df = pd.DataFrame(dataset['train'])



df

df['label'] = df['text'].apply(lambda x: 1 if len(x) > 100 else 0)

df

"""Data Cleaning and Preprocessing"""

def clean_text(text):
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    text = text.lower()  # Convert to lowercase
    text = text.strip()  # Remove leading and trailing spaces
    return text

df['cleaned_text'] = df['text'].apply(clean_text)

df

stop_words = set(stopwords.words('english'))
df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))

df

"""**TF-IDF Encoding and Random Forest Model**"""

# TF-IDF Encoding
vectorizer = TfidfVectorizer(max_features=5000)

X = vectorizer.fit_transform(df["cleaned_text"])

X

y = df["label"]

X_train , X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2 , random_state=42)

# Forest Model
rf_model = RandomForestClassifier(n_estimators=100 ,max_depth=10, n_jobs=-1,  random_state=42)
rf_model.fit(X_train , y_train)

y_pred = rf_model.predict(X_test)

accuracy = accuracy_score(y_test,y_pred)
conf_matrix = confusion_matrix(y_test,y_pred)

print(f"Accuracy:{accuracy}")
print(f"Confusion Matrix:\n{conf_matrix}")

"""# Stage II: Using Deep Learning Models
**Keras Embedding and 1D Convolution Layer**
"""

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense

max_features = 20000
max_len = 100

df.columns

tokenizer = Tokenizer(num_words = max_features)
tokenizer.fit_on_texts(df["cleaned_text"])
sequences = tokenizer.texts_to_sequences(df["cleaned_text"])
X = pad_sequences(sequences , maxlen=max_len)
y=df["label"].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model with Embedding and 1D Convolution Layer
model = Sequential()
model.add(Embedding(max_features, 128, input_length=max_len))
model.add(Conv1D(128, 5, activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Accuracy: {accuracy}")

"""**Stage III: Using BERT Embedding and Any DL Model**"""

import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM
import gensim
from gensim.models import KeyedVectors

df_subset = df.sample(frac=0.1, random_state=42)

sentences = df_subset['cleaned_text'].tolist()
labels = df_subset['label'].tolist()

df_subset



# Tokenize the data
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentences)
word_index = tokenizer.word_index

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentences)
word_index = tokenizer.word_index

data = pad_sequences(sequences, maxlen=100)

labels = to_categorical(np.asarray(labels))

x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense

"""# **Keras Embedding and 1D Convolutional Layer**"""

model_conv1d = Sequential()
model_conv1d.add(Embedding(input_dim=5000, output_dim=128, input_length=100))
model_conv1d.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model_conv1d.add(GlobalMaxPooling1D())
model_conv1d.add(Dense(10, activation='relu'))
model_conv1d.add(Dense(labels.shape[1], activation='softmax'))

model_conv1d.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model_conv1d.summary()

model_conv1d.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))

"""# ***Word2Vec Embedding and LSTM Layer***"""

from gensim.models import Word2Vec
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

tokenized_sentences = [sentence.split() for sentence in sentences]

word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=300, window=5, min_count=1, workers=4)

# Create an embedding matrix
embedding_matrix = np.zeros((len(word_index) + 1, 300))
for word, i in word_index.items():
    if word in word2vec_model.wv:
        embedding_matrix[i] = word2vec_model.wv[word]

# Build the LSTM model
model_lstm = Sequential()
model_lstm.add(Embedding(input_dim=len(word_index) + 1, output_dim=300, weights=[embedding_matrix], input_length=100, trainable=False))
model_lstm.add(LSTM(128))
model_lstm.add(Dense(10, activation='relu'))
model_lstm.add(Dense(labels.shape[1], activation='softmax'))

model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model_lstm.summary()

model_lstm.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))

"""## Comparing Models"""

from sklearn.metrics import confusion_matrix, accuracy_score
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(title)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# Random Forest Model Evaluation
rf_y_pred = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_y_pred)
rf_conf_matrix = confusion_matrix(y_test, rf_y_pred)

print(f"Random Forest Accuracy: {rf_accuracy * 100:.2f}%")
print(f"Random Forest Confusion Matrix:\n{rf_conf_matrix}")

# CNN Model Evaluation
cnn_y_pred = model.predict_classes(X_test)
cnn_accuracy = accuracy_score(y_test, cnn_y_pred)
cnn_conf_matrix = confusion_matrix(y_test, cnn_y_pred)

plot_confusion_matrix(lstm_y_true, lstm_y_pred, 'LSTM Confusion Matrix')

print(f"CNN Model Accuracy: {cnn_accuracy * 100:.2f}%")
print(f"CNN Model Confusion Matrix:\n{cnn_conf_matrix}")

plot_confusion_matrix(lstm_y_true, lstm_y_pred, 'LSTM Confusion Matrix')

# CNN with Word2Vec and LSTM Model Evaluation
lstm_y_pred = np.argmax(model_lstm.predict(x_val), axis=1)
lstm_y_true = np.argmax(y_val, axis=1)
lstm_accuracy = accuracy_score(lstm_y_true, lstm_y_pred)
lstm_conf_matrix = confusion_matrix(lstm_y_true, lstm_y_pred)

print(f"LSTM Model Accuracy: {lstm_accuracy * 100:.2f}%")
print(f"LSTM Model Confusion Matrix:\n{lstm_conf_matrix}")

plot_confusion_matrix(lstm_y_true, lstm_y_pred, 'LSTM Confusion Matrix')

